---
title: "Mapping"
output: html_document
date: "2024-01-11"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning Objectives

1.  **Effectively represent data** using color, symbols, and other visual elements to highlight patterns in categorical data.

2.  Understand how **transforming data improves data visualizations.**

3.  Understand how **different classification schemes can be used to emphasize key aspects of geospatial data.**

4.  **Understand spatial queries** including measurement and relationship queries.

5.  **Perform basic spatial analysis** such as calculating distances, areas, and lengths within spatial datasets.

6.  **Apply proximity analysis techniques** such as creating buffers and identifying nearby features.

------------------------------------------------------------------------

Throughout this workshop series, we will use the following icons:

üîî **Question**: A quick question to help you understand what's going on.

ü•ä **Challenge**: Interactive exercise. We'll go through these in the workshop!

‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.

üí° **Tip**: How to do something a bit more efficiently or effectively.

üìù **Poll**: A zoom poll to help you learn.

üé¨ **Demo**: Showing off something more advanced so you know what you can use R for in the future

------------------------------------------------------------------------

## Improving data visualization with thematic maps

The goal of a thematic map is to use color to visualize the spatial distribution of a variable in order to identify trends, outliers, and the like.

Thematic maps use color to quickly and effectively convey information. For example, maps use brighter or richer colors to signify higher values, and leverage cognitive associations such as mapping water with the color blue. These maps visually communicate spatial patterns, enabling intuitive interpretation and comparison of patterns and data distributions.

Let's compare data on median age, visualized through a standard bar plot vs a thematic plot.

[Load libraries]{.underline}

```{r}

#load the library
library(ggplot2)
```

[Visualize the median age per county as a barplot using ggplot]{.underline}

```{r}

#create a plot of the MED_AGE per county, save as a variable p1
p1 <- ggplot(counties, aes(x = NAME, y = MED_AGE)) +
  geom_col() + #plot the data as bar plots 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # adjust the angle of the x-axis text for easier visualization
p1
```

üîî **Question**: Alameda and San Francisco county neighbor one another, how easy is it to relate the median ages of the two counties using this bar plot?

Visualizing the data using this plot emphasizes numerical relationships, while a thematic map places additional emphasis on the geographic distribution of data. Herein lies a a key benefit of geospatial analyses.

There are three main techniques for improving data visualization via thematic maps:

A. Color palettes

B. Data transformations

C. Classification schemes

Let's use these methods on some thematic maps with ggplot:

### A. Color Palettes

Mapping data geospatially, versus plotting data using XY plot (like the barplot above), adds the additional spatial dimension, via the geometry column, which allows you to visualize how data varies across geographical regions.

`ggplot2` uses the `geom_sf()` function to initiate plotting geospatial data

[Visualize the median age per county as a thematic plot using ggplot]{.underline}

```{r}


p2 <- ggplot(counties, aes(fill = MED_AGE)) + 
  geom_sf() +  # tells ggplot that geographic data are being plotted
  #scale_fill_viridis_c() +
  theme_minimal() + 
  labs(title = "Median Age per County")
p2
```

There are three main types of color palettes (or color maps), each of which has a different purpose: diverging, sequential, and qualitative.

![](http://www.gnuplotting.org/figs/colorbrewer.png)

üí° **Tip**: Sites like [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=Blues&n=3) let's you play around with different types of color maps.

Let us visualize the same data using all 3 types of palettes.

[load the library]{.underline}

```{r}

library(RColorBrewer)
```

To see the names of all color palettes available, try the following command. You may need to enlarge the output image.

```{r}
RColorBrewer::display.brewer.all()
```

1.  **Diverging color palette** - a "diverging" set of colors are used to emphasize mid-range values as well as extremes.

[Plot data with a diverging pallete]{.underline}

```{r}

Div_plot <- ggplot(counties, aes(fill = MED_AGE)) + 
  geom_sf() +
  scale_fill_gradientn(colors = brewer.pal(11, "RdBu")) + # Diverging red-blue color palette
  theme_minimal() + 
  labs(title = "Median Age per County")

Div_plot
```

This is considered a `proportional color map` because the colors are linearly scaled to the data values. The legend has a continuous color ramp rather than discrete data ranges.

2.  **Sequential color palette** - usually uses a single or multi- color hue to emphasize differences in order and magnitude, where darker colors typically mean higher values

Let's plot the MED_AGE counties data using a sequential palette

üîî **Question**: Looking at the script above, what should you change `"RdBu"` to if you wanted a green sequential palette?

-   Hint: you can take a look at the `RColorBrewer::display.brewer.all()` to find the correct syntax

[Plot data with a sequential pallete]{.underline}

```{r}

Seq_plot <- ggplot(counties, aes(fill = MED_AGE)) + 
  geom_sf() +
  scale_fill_gradientn(colors = brewer.pal(9,"Greens")) + # Sequential greens color palette
  theme_minimal() + 
  labs(title = "Median Age per County")
  
Seq_plot
```

üîî **Question**: Do you see any preliminary patterns in this map?

3.  **Qualitative color palette** - a contrasting set of colors to identify distinct categories and avoid implying quantitative significance.

Before we plot using a qualitative color palette, let's do some data preprocessing using the dplyr package.

[load libraries]{.underline}

```{r}

# load the library that will allow subsetting of data
library(dplyr)
```

Here, we will filter for the data where the median age is less than or equal to 32 years, using the `filter()` function. We do this to plot a smaller subset of the data which is easier to visualize than the full dataset. This helps the eye and brain digest the data.

[subse (filter) the data]{.underline}

```{r}

#subset for the counties that have a median age lesser than 32- save as a variable called some_counties
some_counties<- counties %>% filter(MED_AGE<=32)
```

[plot the subsetted data]{.underline}

```{r}

#plot the filtered data
Qual_plot <- ggplot() +
  geom_sf(data = some_counties, aes(fill = NAME)) + #since we are using a qualitative color palette, we cannot plot quantitative data 
  scale_fill_manual(values = brewer.pal(8, "Pastel2")) + #Qualitative pastel color pallette
  theme_minimal() +
  labs(title = "Selected Counties")
Qual_plot
```

üîî **Question**: What is lacking from this qualitative map?

[Overlay plots with ggplot]{.underline}

```{r}

# over the counties geometry onto the subset_counties plot
Qual_plot_overlay <- ggplot() +
  geom_sf(data = counties, color = "black", fill = "transparent") + #add the overall counties geometry
  geom_sf(data = some_counties, aes(fill = NAME)) +
  scale_fill_manual(values = brewer.pal(8, "Pastel2")) + #specify the number of unique colors needed
  theme_minimal() +
  labs(title = "Counties with Median Age < or = 32 Highlighted")
Qual_plot_overlay
```

üîî **Question**: is this a `proportional color map?`

‚ö†Ô∏è **Warning**: When exploring your data as we are doing here, it is important to note that a qualitative color scheme cannot be applied to quantitative data (like the median age). So for this example, we just applied the qualitative scheme on the county names.

During exploratory mapping, it can be helpful to visualize all versions of a map on the same grid. `gridExtra` is one of the packages that allows you to visualize plots in the same image

Let's visualize all 3 plots we just created together:

[Load the library]{.underline}

```{r}

#load the library to help visualize plots together
library(gridExtra)
```

[Visualize multiple plots on one grid]{.underline}

Elements like the title and grid may be useful for individual plots, but cumbersome in joint plots. There are various ways to manipulate plots to add or remove certain elements for improved visualization. In the following example, we remove the plot titles and make the legend smaller, then we combine the plots. Don't get stuck in the syntax for editing the plots!

```{r}

# Remove title from each plot and minimize the legend to prevent overcrowding
Div_plot_1 <- Div_plot  + theme(
  legend.text = element_text(size = 4), #reduce the size of the legend text
  legend.title = element_text(size = 4), #reduce the size of the legend title
  legend.key.size = unit(0.5, "lines"), #reduce the size of the legend itself
  plot.title = element_blank()
)

Seq_plot_1 <- Seq_plot + theme(
  legend.text = element_text(size = 4), #reduce the size of the legend text
  legend.title = element_text(size = 4), #reduce the size of the legend title
  legend.key.size = unit(0.5, "lines"), #reduce the size of the legend itself
  plot.title = element_blank()
)

Qual_plot_overlay_1 <- Qual_plot_overlay + theme(
  legend.text = element_text(size = 4), #reduce the size of the legend text
  legend.title = element_text(size = 4), #reduce the size of the legend title
  legend.key.size = unit(0.5, "lines"), #reduce the size of the legend itself
  plot.title = element_blank()
)

# combine the plots into one grid, with 3 columns
combined_plot <- grid.arrange(Div_plot_1, Seq_plot_1, Qual_plot_overlay_1, ncol=3)

# Print the combined plot
combined_plot


```

üîî **Question**: List some pros and cons of plotting the median age per county using the different color palettes\

Remembers, as a best practice, a **qualitative** color palette should not be used with **quantitative** data and vice versa.

------------------------------------------------------------------------

## ü•ä Challenge 1: Choosing and appropriate color palette

1.  Which types of color palettes would most accurately represent the following data:
    1.  The percent of students that are mixed race ( `MRpct`)
    2.  The school level (e.g. Preschool, Elementary, High) (`SchoolLeve)`
    3.  Whether or not the school is a charter school or not (`Charter`)
    4.  The California Senate District that the school falls in (`SenateCA`), e.g. district 09 etc.
2.  Select the appropriate color palette from R brewer, and plot each data using that palette. Save the plots with the following variable names:
    1.  MRpct_plot
    2.  SchoolLeve_plot
    3.  Charter_plot
    4.  For the SenateCA, the starting code has been provided which filters the data by selecting only a few districts, for easier viewing

‚ö†Ô∏è **Warning**: Some data may be encoded with numbers, but may not be numeric or quantitative data. You can check the type of data using the `class()` function. e.g. `class(scholls_sf$SenateCA)`

[Senate CA]{.underline}

```{r}
# YOUR CODE HERE

```

------------------------------------------------------------------------

There are two major challenges when creating thematic maps:

1.  Our eyes are drawn to the color of larger areas or linear features, even if the values of smaller features are more significant.

2.  The range of data values is rarely evenly distributed across all observations and thus the colors can be misleading as we saw a bit with the Mixed Race Percent plot

Selecting the appropriate color palette can help mitigate these challenges. Sometimes, this alone is not enough. Transforming data can improve the way data values are associated with colors.

------------------------------------------------------------------------

## Transforming Count Data

Data aggregation is where individual-level data e.g. data from people, households, or businesses are summarized into higher geographic levels such as states, counties, or census tracts

Aggregated data such as counts (e.g. the total population in a state) presents a broader overview

To make these counts more comparable across regions, especially those that differ greatly in size, data is transform into normalized variables.

Normalized variables including densities, proportions, or ratios, provide a standardized basis for comparison, allowing for more meaningful understanding of trends and patterns

Let's transform **count** data using **densities**, **proportions**, and **ratios** and visualize the data using `tmap`:

**A. Visualizing data as Counts**

Count data are individual-level data (e.g. population), aggregated by feature (e.g. county)

-   e.g. population within a county

[plot count data using `tmap`]{.underline}

```{r}

# Map of individuals who identify with multiple races
Count_plot <- tm_shape(counties) +
  tm_polygons(col='MULT_RACE', alpha=0.5, #alpha specifies the level of transparency
              palette="Greens", #syntax for specifying color pelettes with tmap plots
              title = "number of multi-race individuals")


Count_plot
```

The data is aggregated by county boundaries and shows the number of people who identify with multiple races per county.

üîî **Question**: What happens if there are more people in a county?

Let's look at the distribution of the data using a histogram.

[plotting data distribution via a histogram]{.underline}

```{r}

#plotting the distribution of people who identify with multiple races per county
hist(counties$MULT_RACE,
     breaks = 40, # number of bins that the range of values is divided into
     main = 'number of multi-race individuals') #title 
```

The distribution shows that many people fall on the lowest end of the distribution, and there seems to be an outlier at the highest end.

Such an uneven distribution is not accurately represented using count data.

The basic cartographic rule is that when mapping data for areas that differ in size you rarely map counts since those differences in size make the comparison less valid or informative.

Data transformations overcome these limitations of count data.

**B. Visualizing data as Densities**

Density data is counts aggregated by feature (county) and normalized by feature area

-   e.g. number of individuals who identify with multiple races per square mile within a county

[Transform data based on area]{.underline}

```{r}

#View the data to identify column containing county area
colnames(counties)
# the "SQ_MILES" column contains the area of each county in square miles

#create a new column that contains the multi_race density data
counties$MULT_RACE_SQ_MILE<-counties$MULT_RACE/counties$SQ_MI

Density_plot<- tm_shape(counties) +
  tm_polygons(col='MULT_RACE_SQ_MILE', alpha=0.5,
              palette="Greens",
              title = "number of multi-race individuals per square mile")
Density_plot
```

üîî **Question**: What are some changes we can make to the map that may highlight certain trends better

[Change the tmap color palette and translucency]{.underline}

```{r}

Density_plot_2<- tm_shape(counties) +
  tm_polygons(col='MULT_RACE_SQ_MILE', alpha=0.8,
              palette="YlOrRd", #recall that sequential color palettes emphasize differences in order and magnitude
              title = "number of multi-race individuals per square mile")
Density_plot_2
```

Remember that is can be helpful to visualize plots side-by-side. The `tmaptools` package has the `tmap_arrange()` function that allows us to do this.

[visualize tmap plots side-by-side]{.underline}

```{r}

#install.packages("tmaptools")
library(tmaptools)

# Assuming you have tmap plots named plot1, plot2, plot3, and plot4
Density_plot_combined <- tmap_arrange(Density_plot, Density_plot_2, ncol = 2)

Density_plot_combined 
```

üîî **Question**: How can you easily view the county(ies) that have a high number of individuals that identify with multiple races?

```{r}
ttm()

Density_plot
```

Normalizing data via densities makes the data more accurate in some cases and more comparable across regions of different sizes. Normalizing data via percentages allows for a direct comparison of the relative contribution, irrespective of size or population.

**C. Visualizing data as Percents/Proportions**

**Proportions / Percentages data** represents data in a specific category divided by the total value across all categories

*e.g. number of individuals that identify with multiple races, as a percent of the total county population*

Now you try it! Plot the 'MULT_RACE counties data as a percent

-   What should you divide `counties$MULT_RACE` to convert it to a percent?
-   Hint: use the total population in 2012

```{r}

#create a new column that contains the multi_race proportion data
counties$MULT_RACE_PERC<-counties$MULT_RACE/counties$POP2012 *100

Percent_plot <- tm_shape(counties) +
  tm_polygons(col='MULT_RACE_PERC', alpha=0.5,
              palette="YlOrRd",
              title = "number of multi-race individuals per total county percent")
Percent_plot
```

## ü•ä Challenge 2: **Visualizing data as Ratios**

**D. Visualizing data as Ratios**

**Rates / Ratios data** represent a value in one category divided by value in another category

1.  Create a new variable "MULT_to_OTHER" that is the ratio of the number of individuals that identify with multiple races to the number of individuals who identify as "OTHER"
2.  plot a histogram to see the distribution
3.  create a plot, named Ratio_plot, of this distribution using `tmap`, and a diverging color palette
4.  Visually identify the county with the highest ratio
5.  combine this and the three previous plots onto one grid

```{r}
# YOUR CODE HERE

```

------------------------------------------------------------------------

## üé¨ Demo:

Often enough, you may want to create a thematic map that requires more colors than are contained within a predefined color palette, like Pastel2 which has a maximum of 8, or you may just want to create your own.

R has a list of colors with their associated color codes that can be manually used.

Let's use this example to plot the different types of school (n=14) using a manual color palette.

[Manually defining a set of colors for a qualitative plot]{.underline}

```{r}

#plot the different types of schools using a manual color palette
schools_plot_2 <- ggplot() +
  geom_sf(data = schools_sf, aes(color = SchoolType)) +  # Use color aesthetic for points
  scale_color_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#ff7f00", "#a65628", "#984ea3", "#999999", "#e41a1c", "#377eb8", "#4daf4a", "#ff7f00", "#a65628", "#984ea3", "#999999")) +  # Custom palette with 14 colors
  theme_minimal() +
  labs(title = "School Type")
print(schools_plot_2)
```

‚ö†Ô∏è **Warning**: plotting qualitative data using readily available color palettes can become limited because each palette contains a predefined number of colors. Also, using too many colors may be overwhelming to the viewing and demphasize important aspects.

------------------------------------------------------------------------

# Classification schemes

Another way to make more meaningful maps is to improve the way in which data values are associated with colors.

The common alternative to the proportional color maps we've created thus far is to use a **classification scheme** to create a **graduated color map**.

A **classification scheme** is a method for binning continuous data values into multiple classes (often 4-7) and then associate those classes with the different colors in a color palette.

The commonly used classification schemes include equal interval, quantiles, natural breaks, heads/tails, and manual schemes. The `?tm_polygons` documentation, under the `style` argument provides keywords names for the different classification styles.

Let's explore each classifications schemes:

[Load libraries]{.underline}

```{r}

library(here) #provides a here() command that builds file paths from the project directory
library(sf)
library(tmap)
```

[Load in datasets]{.underline}

```{r}

#read in the shapefile
counties = st_read(here("data",
                              "california_counties", 
                              "CaliforniaCounties.shp"))

#read in the schools_sf shapefile using a slightly different syntax
schools_sf = st_read(here("data",
                               "California_Schools_2019-20",
                               "SchoolSites1920.shp"))


#data sources: 
# counties: https://gis.data.ca.gov/datasets/8713ced9b78a4abb97dc130a691a8695 
# schools_sf: https://gis.data.ca.gov/datasets/f7f818b0aa7a415192eaf66f192bc9cc
```

**A. Classifying data based on Equal intervals** (e.g. **pretty)**

An Equal-interval classification segments data into equal-size data ranges (e.g., values within 0-10, 10-20, 20-30, etc.)

-   pros:
    -   best for data spread across the entire range of values
    -   easily understood by map readers
-   cons:
    -   avoid if you have highly skewed data or a few big outliers because one or more of the bins may have no data observations

üîî **Question**: Do you recall what transforming count data into density data means?

```{r}

#create a new column that contains the multi_race density data
counties$POP12_SQMI<-counties$POP2012/counties$SQ_MI
```

[plot data using the equal interval classification scheme]{.underline}

```{r}
tmap_mode('plot')
# Plot population density - mile^2
pretty_clasi_plot<- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              alpha = 0.9,
              style = "pretty", #style of the break, pretty = equal interval
              title = "Population Density per mi^2 Equal-Interval Scheme")

pretty_clasi_plot
```

In instances where proportions don't make the data more representative, we can look to other classification schemes.

**B. Classifying data based on Quantiles**

A quantile scheme distributes an equal number of observations in each bin

-   pros:

    -   looks nice, because it best spreads colors across full set of data values
    -   thus, it's often the default scheme for mapping software

-   cons:

    -   the bin ranges are based on the number of observations, not on the data values
    -   thus, different classes can contain very similar or very different data values

We'll now plot the 'POP12_SQMI' counties data using the quantile style

üîî **Question**: What should you change `"pretty"` to?

-   Hint: you can take a look at the `?tm_polygons` documentation to find the correct syntax

[plot data using the quantile classification scheme]{.underline}

```{r}

tmap_mode('plot')
# Plot population density - mile^2
quant_clasi_plot<- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              alpha = 0.9,
              style = "quantile", #style of the break
              title = "Population Density per mi^2 Quantile Scheme") 
quant_clasi_plot
```

**C. Classifying data based on Natural breaks**

Natural breaks minimizes within-class variance and maximize between-class differences. (Don't worry too much about the nuances of each break, the goal is mostly to show you that you have many options).

-   pros:
    -   great for exploratory data analysis, because it can identify natural groupings
-   cons:
    -   class breaks are best fit to one dataset, so the same bins can't always be used for multiple years

[plot data using one type of natural break (fisher)]{.underline}

```{r}
tmap_mode('plot')
# Plot population density - mile^2
nat_clasi_plot <- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              style = "fisher", #style of the break
              alpha = 0.9,
              title = "Population Density per mi^2")
nat_clasi_plot
```

Note the range of each bin.

**D. Classifying data Manually**

user-defined classification schemes allow the user to manually set the breaks for the bins using the `breaks()` argument.

-   pros:
    -   especially useful if you want to slightly change the breaks produced by another scheme
    -   can be used as a fixed set of breaks to compare data over time
-   cons:
    -   more work involved because breaks are made manually

[plot data using a manual classification scheme]{.underline}

```{r}

man_clasi_plot <- tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              style = 'fixed',
              breaks = c(0, 50, 100, 200, 300, 400, max(counties$POP12_SQMI)),
              #labels = c('<50','50 to 100','100 to 200','200 to 300','300 to 400','>400'),
              title = "Population Density per Square Mile Manual Scheme")
man_clasi_plot
```

------------------------------------------------------------------------

## ü•ä Challenge 3: Classifying data based on **Head/Tails**

**D.** Classifying data based on **Head/Tails**

The heads/Tails scheme is tailored to data with a heavy-tailed distributions

1.  search the `?tm_polygons` documentation to find the appropriate argument for a heads/tails classification

2.  create a plot named `HeadsTails_clasi_plot` using a heads/tails scheme

3.  create a combined variable named `combined_clasi_plots` that shows the 4 plots we've now created in pats A-D.

```{r}

# YOUR CODE HERE
```

See the documentation `?classIntervals` or sources such as [Geocomputation with R](https://geocompr.robinlovelace.net/adv-map.html) ebook for more information on data classifications.

Aso note that there are other mapping packages including

-   `mapview`: for a quick and easy interactive map

-   `leaflet`: for highly custom interactive maps that you can output and host on a website

-   `shiny`: for interactive R based applications that use leaflet maps

------------------------------------------------------------------------

## Spatial Queries

-   Spatial queries enable users to explore and map datasets, leading to intricate models and visualizations of real-world features and phenomena not immediately evident in spatial datasets.

-   Spatial queries are a fundamental aspect of spatial analysis that allow for calculations and data subsetting based on spatial relationships between different datasets and geometries, as long as they share geographic space.

-   These relationships operations, like proximity and intersection, play a key role in the analytical process through creating and extracting data metrics and insightful data subsets.

### Spatial Measurement Queries

Measurement queries involve calculations such as distances, areas, and other geometric properties to provide insights into the spatial relationships within a dataset(s).

Ask question like

-   What is feature A's **length**?
    -   What is the length of the BART train line between Walnut Creek and Rockridge?
-   What is feature A's **distance** from feature B?
    -   What is the distance between Berkeley High School and Berkeley BART Station?
-   What is feature A's **area**?
    -   What is the area of Alameda County?

[Load the libraries]{.underline}

```{r, message=F}
library(sf)
library(tmap)
library(here)

```

[Read in the CA Counties data]{.underline}

```{r}

# Read in the counties shapefile
counties = st_read(dsn = here("data", 
                              "california_counties", 
                              "CaliforniaCounties.shp"))

#fix any potential issues with the counties geometry
counties <- st_make_valid(counties)

#avisualize the geometry
plot(counties$geometry)
```

```{r}

#review the contents of the dataframe
head(counties,2)
```

#### Measurement Queries: Units

When conducting measurement queries, it's essential to have consistent units to ensure accurate and meaningful analyses.

Let's start by looking at the different ways units can be expressed and different aspects of spatial data that influence units.

Lets start by selecting data pertaining only to Alameda County and save it to a new spatial dataframe.

\*\* Note that this is an **attribute query** which involves selecting or filtering data based on specific attributes or properties within a dataset

[select only data pertaining to Alameda county]{.underline}

```{r}

#using $ to select for when the NAME column has 'Alameda' as an entry
alameda = counties[counties$NAME=='Alameda',]

#plot the Alameda county geometry
plot(alameda$geometry)
```

A measurement query is similar, but more expansive in that it allow users to subset data and create new relationships based on spatial metrics and calculations (e.g. distances, areas)

We'll start off with some simple measurement queries. Let's first calculate the area of Alameda county using the `st_area()` function.

[Get the area of Alameda County with the`sf` function `st_area`]{.underline}

```{r}

#use the st_area function to get the area of Alameda county
st_area(alameda)
```

This gives the area of the county in square meters. CRS' generally have specific units associated with them.

`sf` uses the `units` package to manage (get and set) units.

It's more useful to return the area of large regions in square KM (or sq miles) and we can do that with the `set_units()` function.

[convert the default units]{.underline}

```{r}

#use the set_units and st_area functions to calculate are and convert the are to km^2 units
units::set_units(st_area(alameda), km^2)
```

‚ö†Ô∏è **Warning**: manual unit conversions (e.g. dividing m^2^ by 10^6^ to get km^2^) using the `sf` package don't translate well - converted values may still be reported as m2.

Now you try it!

Calculate the area of Alameda County in sq miles.

-   What should you change `km^2` to?
-   Hint: you can take a look at [Measurement units in R](https://cran.r-project.org/web/packages/units/vignettes/measurement_units_in_R.html) to get a sense of more units

```{r}

#set the area units to square miles
units::set_units(st_area(alameda), mi^2)  ## WHAT SHOULD YOU CHANGE IT TO?
```

Always check your measurements.

It's a good idea to check one or two measurements before you automate your workflow to make sure you are getting valid values. If we look up the area of Alameda county on [wikipedia](https://en.wikipedia.org/wiki/Alameda_County,_California) (last edited on 7 December 2023) we get 821 sq mi (2130 km2).

üîî **Question**: Are the values returned by `st_area()` valid? Why might they differ?

We can also use `st_area()` to add the area of all counties to the spatial dataframe.

[Create a new column in the existing dataframe containing the area of the counties]{.underline}

```{r}

#create a new column that contains the area in Km^2 for the NAD83 CRS
counties$areakm2_NAD83 <- units::set_units(st_area(counties), km^2)

# take a look at the added column
head(counties)
```

Spatial measurements are greatly dependent on the coordinate reference system (CRS). They can differ greatly depending on the CRS. Let's take a look at the areas in different CRS's using a script with multiple functions.

[Examine the areas using different CRS's]{.underline}

```{r}

#create a new column for each version of the area: 

# Calculate area using data in WGS84 CRS (4326)
counties$areakm2_wgs84 <- units::set_units(st_area(st_transform(counties,4326)), km^2)

# Calculate area using data in UTM NAD83 zone 10 CRS (26910)
counties$areakm2_utm <- units::set_units(st_area(st_transform(counties,26910)), km^2)

# Calculate area using data in Web Mercator CRS (3857)
counties$areakm2_web <- units::set_units(st_area(st_transform(counties, 3857)), km^2)

# Take a look at a subset of the Name and area columns only
head(counties[,c('NAME','areakm2_NAD83','areakm2_wgs84','areakm2_utm','areakm2_web')])

```

Output Interpretation:

-   **NAD83 / CA Albers:** The source data's CRS, CA Albers, is optimized for accurate area measurements within California. Values in the **`area_km2`** column are highly precise, assuming accurate underlying geometry.

-   **WGS84:** Computing areas in WGS84 (a geographic CRS with decimal degrees) yields almost identical values. Modern versions of the **`sf`** package use spherical geometry for precise area computations, even in geographic data.

-   **UTM10:** This CRS is optimized for Northern California, making it less accurate as you move away from the zone's center (e.g., Southern California).

-   **Web Mercator:** While preserving shape, Web Mercator significantly distorts area. It's unsuitable for precise area calculations.

The important takeaway is that you need to use a CRS that is appropriate for your analysis/mapping needs!

When creating a spatial analysis work flow it is common to start by transforming all of your data to the same, appropriate CRS.

------------------------------------------------------------------------

We can use the `st_length()` operator in the same way as `st_area()` to calculate the length of features in a spatial dataframe. Always take note of the output units!

Let's determine how many miles of metro lines there are using Bay Area Rapid Transit (BART) data

[Read in the BART lines data]{.underline}

```{r}

#read in the BART lines data into a new variable
bart_lines <- st_read(here('data', 'transportation', 'bart_lines_2019.geojson'))  #note the different file type

#data source: https://geodata.lib.berkeley.edu/catalog/stanford-mh686mh0418
```

üîî **Question**: what is a quick way to visualize the geospatial data we just read in?

```{r}

#create a quick plot of the bart_lines geometry
plot(bart_lines$geometry)
```

üîî **Question**: what type of geometry do you think the bart_lines data is?

------------------------------------------------------------------------

## ü•ä Challenge 1: Examine and modify length measurements

Similar to the `st_area()` function, the `st_length()` allows you to determine the length of a line geometry.

-   Create new columns called `len_mi`, `len_km`, and `len_m` that contain the length of the bartlines geometry in miles, kilometers, and meters respectfully.

-   Create new columns called len_NAD83, len_WebMarc, and len_WGS84 that contain the length of the bartlines geometry in the NAD83, Web Mercator, and WGS84 CRS's. (search previous text for the respective EPSG codes if needed).

[Examine lengths using different units]{.underline}

```{r}

# YOUR CODE HERE
```

[Examine lengths using different CRS's]{.underline}

```{r}

# YOUR CODE HERE
```

------------------------------------------------------------------------

#### Measurement Queries: Distance

The `st_distance()` function can be used to find the distance between two geometries or two sets of geometries. Let's compute the distance between two schools.

[load in the data]{.underline}

```{r}

# Read in the school_sf shapefile
schools_sf = st_read(dsn =here("data",
                               "California_Schools_2019-20",
                               "SchoolSites1920.shp"))
```

```{r}

#read more about the distance function
?st_distance

colnames(schools_sf) #identify the column name containg the name of the schools
```

[compute the distance between two points]{.underline}

```{r}

#determine the distance between Alameda and Berkeley high
st_distance(schools_sf[schools_sf$SchoolName=='Alameda High',], 
                          schools_sf[schools_sf$SchoolName=='Berkeley High',])
```

You can also use `st_distance()` to find the distance between multiple features

[compute the distance between features in different datasets]{.underline}

```{r}

# tranform the CRS' to match using 2 methods
schools_utm10 <- st_transform(schools_sf, 26910) #assign the CRS based on the EPSG code
bart_lines_utm10 = st_transform(bart_lines, st_crs(schools_utm10 )) #assign the CRS based on the first dataset

View(bart_lines_utm10) #note that there are 6 bart line segments
```

```{r}

#calculate the distance between Berkeley High and each portion of the BART line
st_distance(schools_utm10[schools_utm10$SchoolName=='Berkeley High',], bart_lines_utm10)

```

Note the format of the output. The `st_distance()` calculates the distance from the point location of the school and each segment of the BART lines

üîî **Question**: since the dataframe itself does not contain explicit information about the 6 BART lines, how can one better understand the data?

-   Metadata, including those sometimes contained with the shapefiles may provide additional information.

There are different ways to determine which section of the BART lines to calculate the distance to, for instance, to the nearest point of the line geometry, which depend on the goal of the analysis. See the `?st_distance()` documentation for more details.

Measurement queries focus more on distance, area, etc. relationships between geometrys. Relationship queries relate elements of two geometries.

------------------------------------------------------------------------

### Spatial Relationship Queries

[Spatial relationship queries](https://en.wikipedia.org/wiki/Spatial_relation) consider how two geometries or sets of geometries relate to one another in space.

relationship queries ask questions like:

-   Is feature A **within** feature B?
    -   *What schools are in the city of Berkeley?*
-   Does feature A **intersect** with feature B?
    -   In which cities are Tilden Regional Park located?
-   Does feature A **cross** feature B?
    -   *Does the BART train line cross into Albany?*

![](https://upload.wikimedia.org/wikipedia/commons/5/55/TopologicSpatialRelarions2.png){height="300px"}

These can be used to select features in one dataset based on their spatial relationship to another.

Let's take a look at a few relationship queries.

------------------------------------------------------------------------

1.  Spatial Intersections

Geometry A spatially intersects Geometry B if any of its parts (e.g., a point, line segment, or polygon) is equivalent to, touches, crosses, is contained by, contains, or overlaps any part of Geometry B.

This is the most general of all spatial relationships!

The `st_intersects()` function is used to determine intersections, and can be used to specify explicit relationships (e.g. touches) by setting the operation or `op=()` argument to any of the options listed in the `?st_intersects` help documentation (e.g. `st_touches`, `st_crosses`).

[view help documentation]{.underline}

```{r}

#see the various options similar to intersects. See the Usage section
?st_intersects
```

Let's examine a spatial intersection by determining how continuation schools intersect Alameda county

[determine specific school that intersect one county]{.underline}

```{r}

#convert the counties dataframe into the utm10 CRS
counties_utm10 = st_transform(counties, st_crs(schools_utm10 ))
```

```{r}

#subset only the schools in Alameda county
Alameda_county_utm10 <- counties_utm10[counties_utm10$NAME =="Alameda", ]
```

```{r}

#subset only the Continuing education schools
Continuation_schools_utm10 <- schools_utm10[schools_utm10$SchoolType == "Continuation", ]

#determine the number of continuing education schools there are
nrow(Continuation_schools_utm10)


#more info on Continuation schools: https://www.cde.ca.gov/sp/eo/ce/
```

[use `st_intersects` to determine the intersection]{.underline}

```{r}


# select only the continuing education schools that intersect Alameda county
Continuation_Alameda_utm10 <- Continuation_schools_utm10[Alameda_county_utm10, ,op=st_intersects]

#determine how many schools there are that meet the criteria and view the data on these schools
nrow(Continuation_Alameda_utm10)
head(Continuation_Alameda_utm10)
```

The syntax we used was to

-   Select the features (i.e. rows) in the `Continuation_schools_utm10` dataframe

-   whose geometry **spatially intersects** the `Alameda_county_utm10` geometry

-   and keep all of the other `Continuation_schools_utm10` columns (all- because the extraction brackets have no second argument)

Let's create a quick plot to visualize what our selection returned

[visualize the intersecting schools]{.underline}

```{r}

# create a quick plot of the results the results

plot(Alameda_county_utm10$geometry, border="brown")
plot(Continuation_Alameda_utm10$geometry, col="red", add = T)
```

------------------------------------------------------------------------

## ü•ä Challenge 2: Schools distance

Unlike the `st_intersects()` function, `st_disjoint()` subsets the features in A that do not share space with B. It selects the features that have no spatial intersection.

1.  Subset the counties data by selecting only `"Los Angeles"` in the `counties_utm10` county dataframe. Save this dataframe as `LA_county_utm10`

2.  Subset all the `"K-12"` schools in the `schools_utm10` dataframe. Save this dataframe as `"K_12_schools_utm10"`

3.  Select all K-12 Schools that do not share space with (e.g. NOT in) Los Angeles County

4.  On the same map, plot these schools in `"blue"`, the Los Angeles county boundary in `"purple"`, and add the Alameda county boundary in `"red"`. Using the `plot` function may be helpful.

```{r}

# YOUR CODE HERE

```

------------------------------------------------------------------------

[Here](https://github.com/rstudio/cheatsheets/blob/master/sf.pdf) is an `sf` cheatsheet that lists and briefly explains common relationship functions

![](images/sf_Cheatsheet.png)

Let's expand on our analyses by combining measurement and relationship queries.

------------------------------------------------------------------------

### Measurement and Relationship Queries Combined - Proximity Analyses

Measurement queries return a continuous value (e.g. area) while relationship queries evaluate to true or false, and then return the features for which the relationship is true. Let's take a look at a common analysis that combines those concepts: promiximity analysis.

Proximity analyses ask questions like

-   What schools in Berkeley are within 1/4 mile of a BART station?

Proximity analysis helps identify nearby features---to find all features within a certain maximum distance of features in another dataset

A common workflow for this type of analysis is:

1.  Buffer around the features in the reference dataset to create buffer polygons. (`st_buffer()`)

2.  Run a spatial relationship query to find all features that intersect (or are within) the buffer polygons.

------------------------------------------------------------------------

Let's conduct a proximity analysis to think through the concept of a walkable city. One aspect of a walkable city is that services (d.e public transportation) are accessible within a certain walk time.

Let's consider how 'walkable' Alameda county is by looking at the proximity of BART lines to continuing education schools.

[Plot the geometry]{.underline}

```{r}

#plot the bart_lines geometry using tmap
tm_shape(bart_lines_utm10) + 
  tm_lines(col = 'orange')
```

[Create a buffer]{.underline}

```{r}

#Create a buffer around the bart lines of 800 meters.
bart_lines_buf_utm10 = st_buffer(bart_lines_utm10, dist = 800)

#Note: the value is based on 5-10 minute walk = ~ 4-800 m

#one, somewhat arbitrary, publication on walkable cities: https://www.sciencedirect.com/science/article/pii/S2667091722000188
```

[Visualize schools proximity to BART lines within Alameda county borders]{.underline}

```{r}

#plotting the BART lines 800m buffer and all schools within Alameda
bart_Alameda_plot <- tm_shape(Alameda_county_utm10) + 
  tm_polygons(col = 'lightgrey') + 
tm_shape(bart_lines_utm10) + 
  tm_lines() 

bart_Alameda_plot #creating a plot as a separate variable could make it easier to add to the plot
```

```{r}

#add the 800m bart lines buffer
bart_Alameda_buffer_plot <- bart_Alameda_plot + 
tm_shape(bart_lines_buf_utm10) + 
  tm_polygons(col = 'pink', alpha = 0.5)

bart_Alameda_buffer_plot
```

```{r}

#add the continuing education schools 
bart_Alameda_buffer_continuation_plot <- bart_Alameda_buffer_plot+
tm_shape(Continuation_Alameda_utm10) + 
  tm_dots(col = 'purple', size = 0.2)

bart_Alameda_buffer_continuation_plot
```

Great! Looks like we're all ready to run our spatial relationship query to complete the proximity analysis. At this point (pun intended) we'll select the schools that are in within the BART line buffer polygons.

```{r}

#select the Continuation schools that intersect with the bart lines buffer
Continuation_Alameda_utm10_bart_buf <- Continuation_schools_utm10[bart_lines_buf_utm10, ,op=st_intersects]
```

Now let's overlay again, to see if the schools we selected make sense.

```{r}

# highlight the schools interrsect the BART line buffer in yellow
bart_Alameda_buffer_continuation_intersect_plot <- bart_Alameda_buffer_continuation_plot +  
tm_shape(Continuation_Alameda_utm10_bart_buf) + 
  tm_dots(col = 'yellow', size = 0.2) 
bart_Alameda_buffer_continuation_intersect_plot
```

The schools in yellow would be considered in a 'Walkable City' framework.

üîî **Question**: Take a break and think of other, similar analyses you could perform.

------------------------------------------------------------------------

### Proximity Analysis: Nearest Feature

We can can use `st_distance()` and its companion function `st_nearest_feature()` to compute the distance between each feature of A and the nearest feature in B (e.g. a middle school and the nearest high school).

[subset the schools dataframe]{.underline}

```{r}

#create a new dataframe that only contains schools in the Berkeley Unified school district, 
Berkeley_schools_utm10 <- schools_utm10[schools_utm10$DistrictNa == "Berkeley Unified", ]

#create a new dataframe that contains only the elementary schools in the district
Berkeley_Elementary_schools_utm10 <- Berkeley_schools_utm10[Berkeley_schools_utm10$SchoolType  == "Elementary", ]

#create a new dataframe that contains only the middle schools in the district
Berkeley_Middle_schools_utm10 <- Berkeley_schools_utm10[Berkeley_schools_utm10$SchoolType == "Middle", ]
```

[identify the nearest feature]{.underline}

```{r}

# for each elementary school, extract the ID of the nearest middle school
nearest_elementary_middle_school = st_nearest_feature(Berkeley_Elementary_schools_utm10 , Berkeley_Middle_schools_utm10)

# take a look!
nearest_elementary_middle_school
```

The output: the `st_nearest_feature` function returns the index or ID of the closest feature. These are stored in `nearest_elementary_middle_school`. We can use this index output to see the specific school

[View middle schools using index notation]{.underline}

```{r}

#View what the index output of the st_nearest_features refers to
Berkeley_Middle_schools_utm10[1,]
```

[view syntax of st_nearest_feature function]{.underline}

```{r}

#see help info
?st_nearest_feature
```

We can view the full information of the nearest middle school using the fact that the output is an index.

```{r}

#extract the nearest middle school using indices 

#create a new column in the Berkeley_Elementary_schools_utm10 dataframe called Closest_Middle_School 

View(Berkeley_Middle_schools_utm10[nearest_elementary_middle_school,"SchoolName"])
```

Then we can use `st_distance()` to calculate the distance between each elementary school, and the nearest middle school. To do this, we will create a new column that stores the distance calculation:

[Determine the distance between the elementary school and the nearest middle school]{.underline}

```{r}

#create a new column in the Berkeley_Elementary_schools_utm10 dataframe called elementary_middle_school_dist 

#use st_distance between each elementary school (Berkeley_Elementary_schools_utm10) and the closest middle school (Berkeley_Middle_schools_utm10[nearest_elementary_middle_school,])

Berkeley_Elementary_schools_utm10$elementary_middle_school_dist <- st_distance(
  Berkeley_Elementary_schools_utm10,
  Berkeley_Middle_schools_utm10[nearest_elementary_middle_school,], 
                                              by_element = TRUE)
#remember that nearest_elementary_middle_school is an index that specifies the nearest middle school, which is why we can use it in bracket notation. 

#extract just the school name and the distance
Berkeley_Elementary_schools_utm10[, c("SchoolName","elementary_middle_school_dist")]
                                  
# can order the data
#Berkeley_Elementary_schools_utm10[order(Berkeley_Elementary_schools_utm10$elementary_middle_school_dist),]

```

Note that although we only selected the two columns, the geometry columns came along.

This is one nuanced example of the many of the ways we can work with spatial queries. Use this as an example of what is possible.

# Key Points

-   Thematic maps emphasize the geographic distribution and relationships of data that are not easily discernible using standard plots (like barplots)
-   There are different packages that allow you to visualize geospatial data, using the `plot()` function, or the `tmap` or `ggplot2` packages. Each has different eases of use and different benefits.
-   Understanding your data and knowing what aspects you want to emphasize enables you to utilize different color palettes and transformations to emphasize key aspects of the geographic distribution of the data.
-   Changing the way in which the different shades and colors of a thematic map are associated with data, via binning the data using different classification schemes, enhances the type of information that can be conveyed.
-   Measurement queries involve calculating areas (`st_area() )`,and other geometric properties (`st_length() )` within spatial datasets. We looked at this, for instance, through using **`st_distance()`** to find distances between schools and transportation lines.
-   Relationship queries allow you to analyze how sets of geometries relate to each other in space. We used `st_intersect()` and `st_disjoint()` to look at schools within or outside a county.
-   Proximity analysis allows us to find nearby features (`st_nearest_feature() )` within a certain distance, like identifying schools within a certain distance of transportation lines using buffers (`st_buffer()`)
